!pip install openai==1.27
!pip install langchain==0.1.19
!pip install langchain-openai==0.1.6
!pip install yt_dlp==2024.4.9
!pip install tiktoken==0.6.0
!pip install docarray==0.40.0

import os 
import glob
import openai 
import yt_dlp as youtube_dl
from yt_dlp import DownloadError 
import docarray 

openai_api_key = os.getenv("OPENAI_API_KEY")


# An example YouTube tutorial video
youtube_url = "https://www.youtube.com/watch?v=aqzxYofJ_ck"

# Directory to store the downloaded video
output_dir = "files/audio/"

# Config for youtube-dl
ydl_config = {
    "format": "bestaudio/best",
    "postprocessors": [
        {
            "key": "FFmpegExtractAudio",
            "preferredcodec": "mp3",
            "preferredquality": "192",
        }
    ],
    "outtmpl": os.path.join(output_dir, "%(title)s.%(ext)s"),
    "verbose": True
}


# Check if the output directory exists, if not create it
if not os.path.exists(output_dir): 
    os.makedirs(output_dir)

# Print a message indicating which video is being downloaded
print(f"Downloading video from {youtube_url}")

# Try to download the video using the specified configuration
# If a DownloadError occurs, attempt to download the video again
try: 
    with youtube_dl.YoutubeDL(ydl_config) as ydl: 
        ydl.download([youtube_url])
except DownloadError: 
    with youtube_dl.YoutubeDL(ydl_config) as ydl: 
        ydl.download([youtube_url])



# Find the audio file in the output directory

# Find all the audio files in the output directory
audio_files = glob.glob(os.path.join(output_dir, "*.mp3"))

# Select the first audio file in the list
audio_filename = audio_files[0]

# Print the name of the selected audio file
print(audio_filename)



# Use these settings
audio_file = audio_filename
output_file = "files/transcripts/transcript.txt"
model = "whisper-1"

# Transcribe the audio file to text using OpenAI API
print("converting audio to text...")

# Define an OpenAI client model. Assign to client.
client = openai.OpenAI()

# Open the audio file as read-binary
with open(audio_file, "rb") as audio:
    # Use the model to create a transcription
    response = client.audio.transcriptions.create(file=audio, model=model)

# Extract the transcript from the response
transcript = response.text

# Print the transcript
print(transcript)


# transcript file
# Create the directory for the output file if it doesn't exist
os.makedirs(os.path.dirname(output_file), exist_ok=True)

# Write the transcript to the output file
with open(output_file, "w") as file:
    file.write(transcript)




# From the langchain.document_loaders module, import TextLoader
from langchain.document_loaders import TextLoader

# Create a `TextLoader`, passing the directory of the transcripts. Assign to `loader`.
loader = TextLoader("./files/text")

# Use the TextLoader to load the documents. Assign to docs.
docs = loader.load()

# Show the first element of docs to verify it has been loaded 
docs[0]



# Import the tiktoken package
import tiktoken

# From the langchain.chains module, import RetrievalQA
from langchain.chains import RetrievalQA

# From the langchain_openai package, import ChatOpenAI, OpenAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# From the langchain.vectorstores module, import DocArrayInMemorySearch
from langchain.vectorstores import DocArrayInMemorySearch


# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings
db = DocArrayInMemorySearch.from_documents(
    docs, 
    OpenAIEmbeddings()
)


# Convert the DocArrayInMemorySearch instance to a retriever. Assign to retriever.
retriever = db.as_retriever()

# Create a new ChatOpenAI instance with a temperature of 0.0
llm = ChatOpenAI(temperature = 0.0)


# Create a new RetrievalQA instance with the specified parameters
qa_stuff = RetrievalQA.from_chain_type(
    llm=llm,            # The ChatOpenAI instance to use for generating responses
    chain_type="stuff", # The type of chain to use for the QA system
    retriever=retriever, # The retriever to use for retrieving relevant documents
    verbose=True        # Whether to print verbose output during retrieval and generation
)



# Set the query to be used for the QA system
query = "What is this tutorial about?"

# Invoke the query through the RetrievalQA instance and store the response
response = qa_stuff.invoke(query)

# Print the response to the console
response


# Set the query to be used for the QA system
query = "What is the difference between a training set and test set?"

# Invoke the query through the RetrievalQA instance and store the response
response = qa_stuff.invoke(query)

# Print the response to the console
response


# Set the query to be used for the QA system
query = "Who should watch this lesson?"

# Invoke the query through the RetrievalQA instance and store the response
response = qa_stuff.invoke(query)

# Print the response to the console
response 


# Set the query to be used for the QA system
query = "Who is the greatest football team on earth?"

# Invoke the query through the RetrievalQA instance and store the response
response = qa_stuff.invoke(query)

# Print the response to the console
response



# Set the query to be used for the QA system
query = "How long is the circumference of the earth?"

# Invoke the query through the RetrievalQA instance and store the response
response = qa_stuff.invoke(query)

# Print the response to the console
response 
